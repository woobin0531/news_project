# main.py â€” FastAPI backend (with /api prefix + Dual Query + KoBERT-only optional)
# - React í”„ëŸ°íŠ¸ëŠ” ë² ì´ìŠ¤ë¥¼ http://localhost:8000/api ë¡œ ë§ì¶° í˜¸ì¶œí•˜ì„¸ìš”.
# - KoBERTë§Œ ì„¤ì¹˜í•˜ë©´( transformers, torch ) ìë™ ì‚¬ìš©ë˜ê³ , ë¯¸ì„¤ì¹˜ë©´ ê²½ëŸ‰ í´ë°±ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
# - STT/TTSëŠ” OpenAI APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (melotts ë“± ë¶ˆí•„ìš”)

import os
import io
import re
import ssl
import time
import json
import hashlib
import tempfile
import asyncio
import httpx
from collections import Counter
from datetime import date, timedelta
from typing import List, Dict, Optional, Union

from dotenv import load_dotenv
from bs4 import BeautifulSoup

from fastapi import FastAPI, APIRouter, HTTPException, UploadFile, File, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel

# ë¡œì»¬ ìœ í‹¸
from voice_chat import generate_audio_file         
from patched_cleanre import clean_text

# OpenAI / LangChain
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


# =============================================================================
# í™˜ê²½ë³€ìˆ˜ & í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„
# =============================================================================
load_dotenv()

OPENAI_API_KEY      = os.getenv("OPENAI_API_KEY")
NAVER_CLIENT_ID     = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
if not all([OPENAI_API_KEY, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET]):
    raise ValueError("OPENAI_API_KEY / NAVER_CLIENT_ID / NAVER_CLIENT_SECRET ë¥¼ .envì— ì„¤ì •í•˜ì„¸ìš”.")

openai_client = OpenAI(api_key=OPENAI_API_KEY)

ssl_ctx = ssl.create_default_context()
# ì¼ë¶€ êµ¬í˜• TLS ì‚¬ì´íŠ¸ í˜¸í™˜
ssl_ctx.set_ciphers("DEFAULT@SECLEVEL=1")

# ë¦¬ë‹¤ì´ë ‰íŠ¸ í—ˆìš©
async_http = httpx.AsyncClient(timeout=30.0, verify=ssl_ctx, follow_redirects=True)


# =============================================================================
# FastAPI & CORS
# =============================================================================
app = FastAPI()
app.router.redirect_slashes = True  # /path ì™€ /path/ ëª¨ë‘ í—ˆìš©

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

api = APIRouter(prefix="/api")


# =============================================================================
# ê³µìš© ìœ í‹¸
# =============================================================================
def strip_html(html_string: str) -> str:
    if not html_string:
        return ""
    return BeautifulSoup(html_string, "html.parser").get_text(strip=True)

def purpose_from_keyword(keyword: str) -> str:
    table = {
        "ì½”ì¸": "ê°€ìƒìì‚° ë‰´ìŠ¤ ìš”ì•½",
        "ì£¼ì‹": "ì£¼ì‹ ì‹œì¥ íŠ¸ë Œë“œ ìš”ì•½",
        "ë¶€ë™ì‚°": "ë¶€ë™ì‚° ë‰´ìŠ¤ ìš”ì•½",
        "AI":   "ì¸ê³µì§€ëŠ¥ íŠ¸ë Œë“œ ìš”ì•½",
        "ì·¨ì—…": "ì·¨ì—… ì‹œì¥ ë™í–¥ ìš”ì•½",
    }
    for k, p in table.items():
        if k in keyword:
            return p
    return f"{keyword} ê´€ë ¨ ë‰´ìŠ¤ ìš”ì•½"

def _node_text_len(node) -> int:
    if not node:
        return 0
    for bad in node.select("script, style, noscript, header, footer, nav, aside, form, iframe"):
        bad.decompose()
    return len(BeautifulSoup(str(node), "html.parser").get_text(" ", strip=True))

def _node_text(node) -> str:
    if not node:
        return ""
    for bad in node.select("script, style, noscript, header, footer, nav, aside, form, iframe"):
        bad.decompose()
    text = BeautifulSoup(str(node), "html.parser").get_text(" ", strip=True)
    return " ".join(text.split())

def ensure_bytes_from_generate_audio(result: Union[str, bytes, bytearray]) -> bytes:
    if isinstance(result, (bytes, bytearray)):
        return bytes(result)
    if isinstance(result, str):
        if not os.path.exists(result):
            raise RuntimeError(f"TTS íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {result}")
        with open(result, "rb") as f:
            return f.read()
    raise RuntimeError("ì§€ì›í•˜ì§€ ì•ŠëŠ” TTS ë°˜í™˜ í˜•íƒœ")


# =============================================================================
# ê²½ëŸ‰ í•œêµ­ì–´ í† í°í™”
# =============================================================================
TOKEN_RE = re.compile(r"[ê°€-í£A-Za-z0-9]+")
KOREAN_STOPWORDS = {
    "ê¸°ì","ì‚¬ì§„","ì˜ìƒ","ë„¤ì´ë²„","ì–¸ë¡ ","ì¼ë³´","ë‰´ìŠ¤","ì‹ ë¬¸","ì—°í•©ë‰´ìŠ¤","ì†ë³´",
    "ê·¸ë¦¬ê³ ","í•˜ì§€ë§Œ","ê·¸ëŸ¬ë‚˜","ë•Œë¬¸","ëŒ€í•´","ê´€ë ¨","ìµœê·¼","ì˜¤ëŠ˜","ì´ë²ˆ",
    "ëŒ€í•œ","ì—ì„œëŠ”","ì—ì„œ","ìœ¼ë¡œ","í•˜ë‹¤","í–ˆë‹¤","í–ˆë‹¤ê°€","ì´ë¼","ì´ë‹¤",
    "ê°œì›”","ì˜¤ëŠ˜","ë‚´ì¼","ì§€ë‚œ","ì´ë²ˆ","ì˜¤ëŠ”","í˜„ì¬","ì§€ë‚œí•´","ì˜¬í•´"
}

def tokenize_ko(text: str) -> List[str]:
    text = text or ""
    toks = [t for t in TOKEN_RE.findall(text)]
    return [t for t in toks if len(t) >= 2 and t not in KOREAN_STOPWORDS]

def count_keywords(texts: List[str]) -> Counter:
    c = Counter()
    for t in texts:
        for w in tokenize_ko(t):
            c[w] += 1
    return c

def rerank_links_simple(links: List[Dict], query: str, top_k: int = 5) -> List[Dict]:
    """
    KoBERTê°€ ì—†ì„ ë•Œ ì“°ëŠ” í´ë°± ì¬ì •ë ¬:
      - ì¿¼ë¦¬ í† í°ê³¼ ì œëª© í† í° êµì§‘í•© í¬ê¸°
      - ì œëª© ê¸¸ì´ íŒ¨ë„í‹°(ì§§ì€ ì œëª© ê°€ì )
    """
    if not links:
        return []
    qset = set(tokenize_ko(query))
    scored = []
    for l in links:
        title = l.get("title") or ""
        tset = set(tokenize_ko(title))
        overlap = len(qset & tset)
        length_penalty = 0.15 * max(0, len(tset) - 8)
        score = overlap - length_penalty
        scored.append((score, l))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [l for _, l in scored[:top_k]]


# =============================================================================
# KoBERT ONLY (konlpy ì—†ì´) â€” ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ìë™ í™œì„±í™”
# =============================================================================
import importlib
KOBERT_AVAILABLE = False
try:
    torch = importlib.import_module("torch")
    transformers = importlib.import_module("transformers")
    AutoTokenizer = getattr(transformers, "AutoTokenizer")
    AutoModel = getattr(transformers, "AutoModel")
    _kobert_device = "cuda" if torch.cuda.is_available() else "cpu"
    _kobert_tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1")
    _kobert_model = AutoModel.from_pretrained("skt/kobert-base-v1").to(_kobert_device)
    _kobert_model.eval()
    KOBERT_AVAILABLE = True
except Exception:
    _kobert_device = "cpu"
    _kobert_tokenizer = None
    _kobert_model = None
    KOBERT_AVAILABLE = False

def _kobert_embed(texts: List[str], max_len: int = 128):
    if not KOBERT_AVAILABLE:
        return None
    with torch.no_grad():
        toks = _kobert_tokenizer(
            texts, padding=True, truncation=True, max_length=max_len, return_tensors="pt"
        )
        toks = {k: v.to(_kobert_device) for k, v in toks.items()}
        out = _kobert_model(**toks).last_hidden_state[:, 0, :]  # [CLS]
        out = torch.nn.functional.normalize(out, p=2, dim=1)    # L2 ì •ê·œí™” -> ì½”ì‚¬ì¸ ë‚´ì  ì‚¬ìš©
        return out.cpu()

def rerank_links_kobert(links: List[Dict], query: str, top_k: int = 5) -> List[Dict]:
    if not links or not KOBERT_AVAILABLE:
        return links[:top_k]
    titles = [l["title"] for l in links]
    q = _kobert_embed([query])
    t = _kobert_embed(titles)
    if q is None or t is None:
        return links[:top_k]
    sims = (t @ q[0].unsqueeze(1)).squeeze(1)  # ì •ê·œí™” ë‚´ì  = ì½”ì‚¬ì¸
    order = torch.argsort(sims, descending=True).tolist()
    return [links[i] for i in order[:top_k]]

def _generate_candidates(tokens: List[str], ngram_range=(1,3), max_candidates=1200):
    cands = []
    n1, n2 = ngram_range
    for n in range(n1, n2+1):
        for i in range(0, len(tokens)-n+1):
            phrase = " ".join(tokens[i:i+n])
            if len(phrase) < 2:
                continue
            cands.append(phrase)
            if len(cands) >= max_candidates:
                return list(dict.fromkeys(cands))  # ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°
    return list(dict.fromkeys(cands))

def extract_keywords_kobert(docs: List[str], top_k: int = 12, ngram_range=(1,3)) -> List[tuple]:
    """
    KeyBERT ìŠ¤íƒ€ì¼: ë¬¸ì„œ ì„ë² ë”©(KoBERT CLS) vs í›„ë³´ n-gram ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„.
    konlpy ì—†ì´ regex í† í°ìœ¼ë¡œ í›„ë³´ ìƒì„±, ì„ë² ë”©ì€ KoBERTë§Œ ì‚¬ìš©.
    """
    if not KOBERT_AVAILABLE:
        return []
    doc_text = " ".join(docs)[:4000]
    doc_emb = _kobert_embed([doc_text])
    if doc_emb is None:
        return []

    tokens: List[str] = []
    for d in docs:
        tokens.extend(tokenize_ko(d))
    candidates = _generate_candidates(tokens, ngram_range=ngram_range, max_candidates=1200)
    if not candidates:
        return []

    # ë°°ì¹˜ ì„ë² ë”© í›„ ìœ ì‚¬ë„ ê³„ì‚°
    scores = []
    bs = 64
    for i in range(0, len(candidates), bs):
        emb = _kobert_embed(candidates[i:i+bs])
        if emb is None:
            continue
        sims = (emb @ doc_emb[0].unsqueeze(1)).squeeze(1).tolist()
        scores.extend(sims)

    # í›„ë³´ë³„ ìµœê³  ì ìˆ˜ ìœ ì§€ í›„ ìƒìœ„ top_k
    best = {}
    for cand, sc in zip(candidates, scores):
        if cand not in best or sc > best[cand]:
            best[cand] = sc
    ranked = sorted(best.items(), key=lambda x: x[1], reverse=True)[:top_k]
    return ranked  # [(phrase, score), ...]


# =============================================================================
# ëª¨ë¸ & ì²´ì¸ (LLM í”„ë¡¬í”„íŠ¸)
# =============================================================================
class NewsTrendRequest(BaseModel):
    keyword: str

class TTSRequest(BaseModel):
    text: str

llm = ChatOpenAI(model="gpt-4o", temperature=0.7, api_key=OPENAI_API_KEY)
output_parser = StrOutputParser()

# HTML â†’ ë³¸ë¬¸ ì¶”ì¶œ (LLM í´ë°±)
extraction_chain = (
    PromptTemplate.from_template(
        "You are an expert web scraper. Your task is to extract the main article text from the given HTML content. "
        "Ignore all HTML tags, scripts, styles, advertisements, navigation bars, headers, footers, related links, and comments. "
        "Return only the clean, plain text of the main news article in Korean.\n\n"
        "## HTML Content:\n{html_content}"
    ) | llm | output_parser
)

# ì´ì¤‘ ì¿¼ë¦¬ ìƒì„±: q1(í¬ê´„) + q2(êµ¬ì²´)
dual_query_chain = (
    PromptTemplate.from_template(
        "ë„ˆëŠ” í•œêµ­ì–´ ë‰´ìŠ¤ ê²€ìƒ‰ ì „ëµê°€ì•¼. ì‚¬ìš©ìì˜ ì›ë¬¸ ì§ˆì˜ì™€ ì´ˆê¸° ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•œ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ "
        "1) í¬ê´„ì ì¸ 1ì°¨ ê²€ìƒ‰ì–´(q1)ì™€ 2) êµ¬ì²´ì ì´ê³  ì•¡ì…˜ ì¤‘ì‹¬ì˜ 2ì°¨ ê²€ìƒ‰ì–´(q2)ë¥¼ ë§Œë“¤ì–´ë¼.\n"
        "- q1ì€ ì£¼ì œë¥¼ ë„“ê²Œ í¬ì°©í•´ ìƒìœ„ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•˜ê¸° ì¢‹ê²Œ ë§Œë“ ë‹¤.\n"
        "- q2ëŠ” ì‹œê¸°(ìµœê·¼/ì˜¬í•´/ì§€ë‚œë‹¬ ë“±), ëŒ€ìƒ(êµ­ê°€/ê¸°ì—…/ì¸ë¬¼), í–‰ìœ„(í˜‘ìƒ/í•©ì˜/ì¸í•˜/ì œì¬/ë°œí‘œ ë“±), í’ˆëª©(ì² ê°•/ìë™ì°¨/ë°˜ë„ì²´ ë“±)ì„ ëª…ì‹œí•œë‹¤.\n"
        "ì¶œë ¥ì€ JSONë§Œ í—ˆìš©í•œë‹¤. ë‹¤ë¥¸ ì„¤ëª… ê¸ˆì§€.\n"
        '{{"q1":"...","q2":"..."}}\n\n'
        "## ì‚¬ìš©ì ì§ˆì˜\n{user_query}\n\n"
        "## ìƒìœ„ í‚¤ì›Œë“œ(ë¹ˆë„ìˆœ)\n{top_keywords}\n"
    ) | llm | output_parser
)

# ê¸°ì‚¬ ìš”ì•½
summary_chain = (
    PromptTemplate.from_template(
        "ë„ˆëŠ” í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” AIì•¼.\nìš”ì•½ ëª©ì : {purpose}\në‰´ìŠ¤ ê¸°ì‚¬ ì›ë¬¸: {article}"
    ) | llm | output_parser
)

# íŠ¸ë Œë“œ ì¢…í•©
trend_chain = (
    PromptTemplate.from_template(
        "ë‹¤ìŒì€ '{purpose}'ì™€ ê´€ë ¨ëœ ì—¬ëŸ¬ ê¸°ì‚¬ ìš”ì•½ë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì‹  íŠ¸ë Œë“œë¥¼ 3~5ì¤„ë¡œ ì¢…í•©í•´ì¤˜.\n\n## ê¸°ì‚¬ ìš”ì•½ë“¤:\n{summaries}"
    ) | llm | output_parser
)


# =============================================================================
# ë„¤ì´ë²„ API + ë³¸ë¬¸ ì¶”ì¶œ
# =============================================================================
async def search_news_titles(keyword: str, max_items: int = 3) -> List[Dict]:
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    params = {"query": keyword, "display": max_items, "start": 1, "sort": "date"}
    try:
        res = await async_http.get(url, headers=headers, params=params)
        res.raise_for_status()
        items = res.json().get("items", [])
        out = []
        seen = set()
        for it in items:
            title = strip_html(it.get("title", ""))
            link = it.get("link") or ""
            if not title or not link or link in seen:
                continue
            seen.add(link)
            out.append({"title": title, "url": link})
        return out
    except httpx.HTTPStatusError as e:
        raise HTTPException(status_code=e.response.status_code, detail="ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ ì‹¤íŒ¨")
    except Exception:
        raise HTTPException(status_code=500, detail="ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì„œë²„ ì˜¤ë¥˜")

async def extract_article_text(url: str, sem: asyncio.Semaphore) -> str:
    try:
        headers = {
            "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                           "(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
        }
        res = await async_http.get(url, headers=headers)
        res.raise_for_status()
        soup = BeautifulSoup(res.content, "html.parser")

        known = [
            "#dic_area", "#articleBodyContents", ".article_body",
            ".newsct_body", ".article_view_body", "article"
        ]
        content = next((soup.select_one(s) for s in known if soup.select_one(s)), None)

        if content and _node_text_len(content) >= 300:
            return _node_text(content)

        if not content:
            for tag in soup(["script", "style", "header", "footer", "nav", "aside", "form", "iframe"]):
                tag.decompose()
            body = soup.body
            if not body:
                return ""
            if _node_text_len(body) >= 300:
                return _node_text(body)
            html_snippet = str(body)
        else:
            html_snippet = str(content)

        MAX = 15000
        if len(html_snippet) > MAX:
            chunks = [html_snippet[i:i+MAX] for i in range(0, len(html_snippet), MAX)]
            out_parts = []
            for ch in chunks:
                async with sem:
                    out_parts.append(await extraction_chain.ainvoke({"html_content": ch}))
            return "\n".join(out_parts)
        else:
            async with sem:
                return await extraction_chain.ainvoke({"html_content": html_snippet})
    except Exception:
        return ""


# =============================================================================
# ì¸ê¸° í‚¤ì›Œë“œ (ë„¤ì´ë²„ DataLab)
# =============================================================================
DEFAULT_KEYWORD_GROUPS = [
    {"groupName": "ì½”ì¸",   "keywords": ["ì½”ì¸", "ë¹„íŠ¸ì½”ì¸", "ì´ë”ë¦¬ì›€"]},
    {"groupName": "ë¶€ë™ì‚°", "keywords": ["ë¶€ë™ì‚°", "ì•„íŒŒíŠ¸", "ì „ì„¸"]},
    {"groupName": "ì£¼ì‹",   "keywords": ["ì£¼ì‹", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥"]},
    {"groupName": "AI",    "keywords": ["AI", "ì¸ê³µì§€ëŠ¥", "ì±—GPT"]},
    {"groupName": "í…ŒìŠ¬ë¼", "keywords": ["í…ŒìŠ¬ë¼", "ì¼ë¡  ë¨¸ìŠ¤í¬"]},
]

async def fetch_popular_keywords(days: int = 7, top_k: int = 8, groups: Optional[List[Dict]] = None) -> List[Dict]:
    groups = groups or DEFAULT_KEYWORD_GROUPS
    start = (date.today() - timedelta(days=days)).strftime("%Y-%m-%d")
    end   = date.today().strftime("%Y-%m-%d")

    url = "https://openapi.naver.com/v1/datalab/search"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        "Content-Type": "application/json",
    }
    payload = {"startDate": start, "endDate": end, "timeUnit": "date",
               "keywordGroups": groups, "device": "", "ages": [], "gender": ""}

    try:
        res = await async_http.post(url, headers=headers, json=payload)
        res.raise_for_status()
        data = res.json()
        results = []
        for item in data.get("results", []):
            score = sum(d.get("ratio", 0.0) for d in item.get("data", []))
            results.append({"keyword": item.get("title"), "score": round(score, 2)})
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]
    except httpx.HTTPStatusError as e:
        raise HTTPException(status_code=e.response.status_code, detail="ì¸ê¸° í‚¤ì›Œë“œ ì¡°íšŒ ì‹¤íŒ¨")
    except Exception:
        raise HTTPException(status_code=500, detail="ì¸ê¸° í‚¤ì›Œë“œ ì¡°íšŒ ì¤‘ ì„œë²„ ì˜¤ë¥˜")


# =============================================================================
# API ì—”ë“œí¬ì¸íŠ¸ (/api/*)
# =============================================================================
@api.get("/headline_quick")
async def headline_quick(kw: str = Query(..., min_length=1)):
    """ìƒë‹¨ ì¹´ë“œìš© ë¹ ë¥¸ í•œ ê±´ ìš”ì•½ â€” ìƒìœ„ Nê°œ ì¤‘ 10ë¶„ ë‹¨ìœ„ íšŒì „."""
    items = await search_news_titles(kw, max_items=5)
    if not items:
        raise HTTPException(status_code=404, detail="ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ")

    # 10ë¶„ ë‹¨ìœ„ë¡œ í‚¤ì›Œë“œë³„ íšŒì „ (í•­ìƒ ê°™ì€ ê¸°ì‚¬ ë°©ì§€)
    seed = f"{kw}:{int(time.time() // 600)}"
    idx = int(hashlib.md5(seed.encode()).hexdigest(), 16) % len(items)
    pick = items[idx]

    sem = asyncio.Semaphore(4)
    article = await extract_article_text(pick["url"], sem)
    article_clean = clean_text(article, "KR") if article else ""

    # ì˜¤íƒ ê°€ë“œ(ë³¸ë¬¸ ë¶€ì‹¤ / í”„ë¡¬í”„íŠ¸ ì”í–¥)
    suspicious = ["You are an expert web scraper", "HTML", "ë³¸ë¬¸ì„ ì¶”ì¶œ", "ë„¤ë¹„ê²Œì´ì…˜", "í—¤ë”", "í‘¸í„°"]
    if (not article_clean) or (len(article_clean) < 200) or any(s in article_clean for s in suspicious):
        return {"title": pick["title"], "url": pick["url"], "summary": "ìš”ì•½ ì‹¤íŒ¨: ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    summary = await summary_chain.ainvoke({"purpose": purpose_from_keyword(kw), "article": article_clean})
    return {"title": pick["title"], "url": pick["url"], "summary": summary}


@api.post("/news_trend")
async def news_trend(req: NewsTrendRequest):
    """
    ê°•í™”ëœ ì´ì¤‘ ì¿¼ë¦¬ íŒŒì´í”„ë¼ì¸(ì¶”ê°€ ì„¤ì¹˜ ì—†ì´ ë™ì‘, KoBERT ìˆìœ¼ë©´ ìë™ ì‚¬ìš©):
      1) q1(í¬ê´„) = ì‚¬ìš©ì ì›ë¬¸ â†’ q1 ê²€ìƒ‰(ìµœëŒ€ 8ê±´) â†’ ì¼ë¶€ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ â†’ ê²½ëŸ‰ í† í°í™” ê¸°ë°˜ í‚¤ì›Œë“œ ì§‘ê³„
      2) (q1 ê²°ê³¼ì˜ Top í‚¤ì›Œë“œ/KOBERT í‚¤ì›Œë“œ) + ì›ì§ˆì˜ â†’ LLMì´ q1/q2(JSON) ìƒì„±
      3) q2 ê²€ìƒ‰(ìµœëŒ€ 8ê±´) â†’ KoBERT ì¬ì •ë ¬(ê°€ëŠ¥ì‹œ) / ê²½ëŸ‰ í† í° êµì§‘í•© ì¬ì •ë ¬(í´ë°±) â†’ ìƒìœ„ 3ê±´ ë³¸ë¬¸ ìš”ì•½
      4) íŠ¸ë Œë“œ ì¢…í•© + í‚¤ì›Œë“œ í†µê³„ ë°˜í™˜
    """
    user_query = (req.keyword or "").strip()
    if not user_query:
        raise HTTPException(status_code=400, detail="keyword is required")

    sem = asyncio.Semaphore(6)

    # (A) 1ì°¨ ì¿¼ë¦¬
    q1_seed = user_query
    q1_links = await search_news_titles(q1_seed, max_items=8)
    if not q1_links:
        raise HTTPException(status_code=404, detail="ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 1ì°¨ ê¸°ì‚¬ ìƒ˜í”Œ (ìµœëŒ€ 5ê±´)
    sample_targets = q1_links[:5]
    q1_articles = await asyncio.gather(*[extract_article_text(l["url"], sem) for l in sample_targets])

    cleaned_samples = [clean_text(a, "KR") for a in q1_articles if a]

    # ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ
    kw_counter = count_keywords(cleaned_samples)
    topkw = kw_counter.most_common(12)

    # KoBERT ì¶”ì¶œ í‚¤ì›Œë“œ(ì„¤ì¹˜ëœ ê²½ìš°)
    kobert_keywords = extract_keywords_kobert(cleaned_samples, top_k=12) if KOBERT_AVAILABLE else []

    # (B) LLMìœ¼ë¡œ q1/q2 ìƒì„± â€” KoBERT í‚¤ì›Œë“œ ìš°ì„  ë°˜ì˜
    top_for_dual = (
        ", ".join([k for k, _ in kobert_keywords[:10]]) or
        ", ".join([w for w, _ in topkw[:10]]) or
        "(ì—†ìŒ)"
    )
    dq_json = await dual_query_chain.ainvoke({
        "user_query": user_query,
        "top_keywords": top_for_dual
    })
    try:
        d = json.loads(dq_json)
        q1 = (d.get("q1") or q1_seed).strip()
        q2 = (d.get("q2") or q1_seed).strip()
    except Exception:
        q1 = q1_seed
        q2 = q1_seed

    # (C) 2ì°¨ ê²€ìƒ‰ + ì¬ì •ë ¬
    q2_links_raw = await search_news_titles(q2, max_items=8)
    if KOBERT_AVAILABLE and q2_links_raw:
        q2_links = rerank_links_kobert(q2_links_raw, q2, top_k=5)
    else:
        q2_links = rerank_links_simple(q2_links_raw or [], q2, top_k=5)

    if not q2_links:
        if KOBERT_AVAILABLE:
            q2_links = rerank_links_kobert(q1_links, q1, top_k=5)
        else:
            q2_links = rerank_links_simple(q1_links, q1, top_k=5)

    final_pick = (q2_links or q1_links)[:3]

    # ë³¸ë¬¸ ì¶”ì¶œ â†’ ìš”ì•½
    final_articles = await asyncio.gather(*[extract_article_text(l["url"], sem) for l in final_pick])
    purpose = purpose_from_keyword(f"{user_query}, {q2}")

    summaries = await asyncio.gather(
        *[
            summary_chain.ainvoke({"purpose": purpose, "article": clean_text(a, "KR")})
            if a and a.strip()
            else asyncio.sleep(0, result="ìš”ì•½ ì‹¤íŒ¨: ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            for a in final_articles
        ]
    )
    details = [{"title": link["title"], "url": link["url"], "summary": summ}
               for link, summ in zip(final_pick, summaries)]

    trend_input = "\n\n".join(s for s in summaries if "ìš”ì•½ ì‹¤íŒ¨" not in s)
    trend = await trend_chain.ainvoke({"purpose": purpose, "summaries": trend_input}) if trend_input else ""
    if (not trend.strip()) and not any("ìš”ì•½ ì‹¤íŒ¨" not in d["summary"] for d in details):
        trend = "ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì°¾ì•˜ì§€ë§Œ, ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”."

    return {
        "initial_keyword": user_query,
        "q1": q1,
        "q2": q2,
        "refined_keyword": q2,  # í”„ë¡ íŠ¸ í˜¸í™˜
        "purpose": purpose,
        "trend_digest": trend,
        "trend_articles": details,
        "keyword_stats": [{"token": w, "count": c} for w, c in topkw],
        "keyword_extractive": (
            [{"token": k, "score": round(float(s), 4)} for k, s in kobert_keywords] if kobert_keywords else []
        ),
        "debug": {
            "rerank": "kobert" if KOBERT_AVAILABLE else "token-overlap",
            "kobert": KOBERT_AVAILABLE
        }
    }


@api.get("/popular_keywords")
async def popular_keywords(days: int = 7, top_k: int = 8) -> List[Dict]:
    return await fetch_popular_keywords(days=days, top_k=top_k, groups=DEFAULT_KEYWORD_GROUPS)


@api.post("/generate-tts")
async def generate_tts(req: TTSRequest):
    text = (req.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="text is required")
    try:
        result = generate_audio_file(text)         # path or bytes
        audio_bytes = ensure_bytes_from_generate_audio(result)
        return StreamingResponse(io.BytesIO(audio_bytes), media_type="audio/mpeg",
                                 headers={"Cache-Control": "no-store"})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"TTS ì˜¤ë¥˜: {e}")


# ============================
# ğŸš€ STT ì•ˆì •í™”(ê°€ë“œ + í´ë°±) ë²„ì „
# ============================
@api.post("/generate-stt")
async def generate_stt(file: UploadFile = File(...)):
    try:
        content = await file.read()

        # 1) ë¹ˆ/ì§§ì€ íŒŒì¼ ê°€ë“œ (ì£¼ë¡œ ê¶Œí•œ/ë…¹ìŒ ì‹¤íŒ¨)
        if not content or len(content) < 2048:  # 2KB ë¯¸ë§Œì€ ì‚¬ì‹¤ìƒ ë¹ˆ ì˜¤ë””ì˜¤
            return JSONResponse(
                {"text": "", "error": f"audio too small: {len(content)} bytes"},
                status_code=400
            )

        suffix = os.path.splitext(file.filename or "")[-1] or ".webm"
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
        try:
            tmp.write(content)
            tmp.flush()
            tmp.close()

            # 2) 1ì°¨ ëª¨ë¸ ì‹œë„: gpt-4o-mini-transcribe (ê³„ì •/ë¦¬ì „ì— ë”°ë¼ ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆìŒ)
            try:
                with open(tmp.name, "rb") as fh:
                    result = openai_client.audio.transcriptions.create(
                        model="gpt-4o-mini-transcribe",
                        file=fh,
                    )
            except Exception:
                # 3) 2ì°¨ í´ë°±: whisper-1
                with open(tmp.name, "rb") as fh:
                    result = openai_client.audio.transcriptions.create(
                        model="whisper-1",
                        file=fh,
                    )

            text = getattr(result, "text", "") or ""
            return JSONResponse({"text": text})

        finally:
            try:
                os.unlink(tmp.name)
            except Exception:
                pass

    except Exception as e:
        # í”„ëŸ°íŠ¸ì—ì„œ ì›ì¸ íŒŒì•…ì„ ì‰½ê²Œ í•˜ë ¤ê³  ë©”ì‹œì§€ ê·¸ëŒ€ë¡œ ë…¸ì¶œ
        return JSONResponse({"text": "", "error": str(e)}, status_code=500)


# ë¼ìš°í„° ë“±ë¡
app.include_router(api)


# =============================================================================
# ì¢…ë£Œ í›…
# =============================================================================
@app.on_event("shutdown")
async def _shutdown():
    await async_http.aclose()
